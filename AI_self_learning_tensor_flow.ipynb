{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI self learning tensor flow.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhmIxYaC9AWNFopAAm/jc+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyambhatt5/Satyam/blob/main/AI_self_learning_tensor_flow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMenRc0AS21T"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P4qyA7_TsZG"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDQlbv7MT3Le"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZOkNeMeUCyQ"
      },
      "source": [
        "tf.keras.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riS5eqL3UTPi"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "#https://keras.io/api/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtCKbyDGAuzV"
      },
      "source": [
        "(x_train,y_train),(x_test,y_test)=mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcrpYABUBDYk"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doSd7SABmTDP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFjPj_EsCuUo"
      },
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vinoOGWD2xw"
      },
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R-yx9ZxEAEj"
      },
      "source": [
        "plt.imshow(x_train[1500],cmap=\"gray\",aspect=\"auto\")  #image \n",
        "\n",
        "print(y_train[1500]) #label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56gTiZIGEKAy"
      },
      "source": [
        "x_train[1500].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-faqS5QJ_uc"
      },
      "source": [
        "print(x_train[1500])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Inqbr-fKMmv"
      },
      "source": [
        "\n",
        "#now we have two problem  \n",
        "\n",
        "#-convert the image into the 0 to 1 \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "alist= np.array([1,5,10,100,200,255])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvE97TrzPozJ"
      },
      "source": [
        "alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgAtf1BRQ7RD"
      },
      "source": [
        "#now the rnage data into 0 to 1 let divide the data \n",
        "\n",
        "alist=alist/255\n",
        "alist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzGSpjUhRWX4"
      },
      "source": [
        "x_train=x_train/255  #using min max scaller in for given data \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR1S2Wb5Ryy4"
      },
      "source": [
        "x_train[1500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-hHNT_JSJD7"
      },
      "source": [
        "#now neural network says that i cannot handle this problem \n",
        "#how to send the 2 d images? \n",
        "#it will send move when we convert 2 d image into the i d image \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFgYY0irbY-n"
      },
      "source": [
        "#for example function we use numpy \n",
        "\n",
        "blist=np.array([1,2,0,1,2,0,1,2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZsGYCh8RkAS"
      },
      "source": [
        "#use one hot code \n",
        "#how ?\n",
        "#now is convert into the unique value then  it should be fix the position \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFnvxeVBbyCV"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#use one HOT Code \n",
        "#how ?\n",
        "#now is convert into the unique value then  it should be fix the position \n",
        "\n",
        "tf.keras.utils.to_categorical(blist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaVSQs-2caff"
      },
      "source": [
        "#let use same tchnique in mnist \n",
        "\n",
        "\n",
        "y_train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy95kAMsdphc"
      },
      "source": [
        "\r\n",
        "y_train_p=tf.keras.utils.to_categorical(y_train)\r\n",
        "y_train_p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAM_axupd5Yw"
      },
      "source": [
        "\n",
        "\n",
        "#from devloping the \"model\" \n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "#>devloping the \"layers\"\n",
        "\n",
        "from tensorflow.keras.layers import Dense,Flatten #it is to convert the 2d image into the 1 d image \n",
        "\n",
        "\n",
        "#>for building  the \"Activation function\"\n",
        "\n",
        "from tensorflow.keras.activations import relu,softmax #soft max is use for the prediction \n",
        "#relu use for the inner and soft max use for the outer layer function \n",
        "\n",
        "#>find the \"loss\" of the function \n",
        "\n",
        "from tensorflow.keras.losses import categorical_crossentropy,binary_crossentropy\n",
        "\n",
        "#> find the \"optimizer\"\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7TvX6hRhi0v"
      },
      "source": [
        "\n",
        "#Devloping the Model \n",
        "\n",
        "#call the model\n",
        "model=Sequential()\n",
        "\n",
        "#adding the first layer  it  is convert 2d image to 1d image \n",
        "model.add(Flatten(input_shape=(28,28)))\n",
        "\n",
        "#adding the second layer (Hidden Image)\n",
        "model.add(Dense(256,activation='relu'))\n",
        "\n",
        "#adding the Output layer \n",
        "model.add(Dense(10,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_lBno972noF"
      },
      "source": [
        "#over all Summery\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yuf622In4JaU"
      },
      "source": [
        "#compile the model with other function\n",
        "\n",
        "model.compile(optimizer=SGD(),loss=binary_crossentropy,metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9slnmkJBBSkR"
      },
      "source": [
        "#Fit the model build up and Run\n",
        "model.fit(x_train,y_train_p,epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFY2E5RcDzOp"
      },
      "source": [
        "Signature: model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
        "Source:   \n",
        "  @enable_multi_worker\n",
        "  def fit(self,\n",
        "          x=None,\n",
        "          y=None,\n",
        "          batch_size=None,\n",
        "          epochs=1,\n",
        "          verbose=1,\n",
        "          callbacks=None,\n",
        "          validation_split=0.,\n",
        "          validation_data=None,\n",
        "          shuffle=True,\n",
        "          class_weight=None,\n",
        "          sample_weight=None,\n",
        "          initial_epoch=0,\n",
        "          steps_per_epoch=None,\n",
        "          validation_steps=None,\n",
        "          validation_batch_size=None,\n",
        "          validation_freq=1,\n",
        "          max_queue_size=10,\n",
        "          workers=1,\n",
        "          use_multiprocessing=False):\n",
        "    \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n",
        "\n",
        "    Arguments:\n",
        "        x: Input data. It could be:\n",
        "          - A Numpy array (or array-like), or a list of arrays\n",
        "            (in case the model has multiple inputs).\n",
        "          - A TensorFlow tensor, or a list of tensors\n",
        "            (in case the model has multiple inputs).\n",
        "          - A dict mapping input names to the corresponding array/tensors,\n",
        "            if the model has named inputs.\n",
        "          - A `tf.data` dataset. Should return a tuple\n",
        "            of either `(inputs, targets)` or\n",
        "            `(inputs, targets, sample_weights)`.\n",
        "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
        "            or `(inputs, targets, sample_weights)`.\n",
        "          A more detailed description of unpacking behavior for iterator types\n",
        "          (Dataset, generator, Sequence) is given below.\n",
        "        y: Target data. Like the input data `x`,\n",
        "          it could be either Numpy array(s) or TensorFlow tensor(s).\n",
        "          It should be consistent with `x` (you cannot have Numpy inputs and\n",
        "          tensor targets, or inversely). If `x` is a dataset, generator,\n",
        "          or `keras.utils.Sequence` instance, `y` should\n",
        "          not be specified (since targets will be obtained from `x`).\n",
        "        batch_size: Integer or `None`.\n",
        "            Number of samples per gradient update.\n",
        "            If unspecified, `batch_size` will default to 32.\n",
        "            Do not specify the `batch_size` if your data is in the\n",
        "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
        "            (since they generate batches).\n",
        "        epochs: Integer. Number of epochs to train the model.\n",
        "            An epoch is an iteration over the entire `x` and `y`\n",
        "            data provided.\n",
        "            Note that in conjunction with `initial_epoch`,\n",
        "            `epochs` is to be understood as \"final epoch\".\n",
        "            The model is not trained for a number of iterations\n",
        "            given by `epochs`, but merely until the epoch\n",
        "            of index `epochs` is reached.\n",
        "        verbose: 0, 1, or 2. Verbosity mode.\n",
        "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
        "            Note that the progress bar is not particularly useful when\n",
        "            logged to a file, so verbose=2 is recommended when not running\n",
        "            interactively (eg, in a production environment).\n",
        "        callbacks: List of `keras.callbacks.Callback` instances.\n",
        "            List of callbacks to apply during training.\n",
        "            See `tf.keras.callbacks`.\n",
        "        validation_split: Float between 0 and 1.\n",
        "            Fraction of the training data to be used as validation data.\n",
        "            The model will set apart this fraction of the training data,\n",
        "            will not train on it, and will evaluate\n",
        "            the loss and any model metrics\n",
        "            on this data at the end of each epoch.\n",
        "            The validation data is selected from the last samples\n",
        "            in the `x` and `y` data provided, before shuffling. This argument is\n",
        "            not supported when `x` is a dataset, generator or\n",
        "           `keras.utils.Sequence` instance.\n",
        "        validation_data: Data on which to evaluate\n",
        "            the loss and any model metrics at the end of each epoch.\n",
        "            The model will not be trained on this data. Thus, note the fact\n",
        "            that the validation loss of data provided using `validation_split`\n",
        "            or `validation_data` is not affected by regularization layers like\n",
        "            noise and dropuout.\n",
        "            `validation_data` will override `validation_split`.\n",
        "            `validation_data` could be:\n",
        "              - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
        "              - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
        "              - dataset\n",
        "            For the first two cases, `batch_size` must be provided.\n",
        "            For the last case, `validation_steps` could be provided.\n",
        "            Note that `validation_data` does not support all the data types that\n",
        "            are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
        "        shuffle: Boolean (whether to shuffle the training data\n",
        "            before each epoch) or str (for 'batch'). This argument is ignored\n",
        "            when `x` is a generator. 'batch' is a special option for dealing\n",
        "            with the limitations of HDF5 data; it shuffles in batch-sized\n",
        "            chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
        "        class_weight: Optional dictionary mapping class indices (integers)\n",
        "            to a weight (float) value, used for weighting the loss function\n",
        "            (during training only).\n",
        "            This can be useful to tell the model to\n",
        "            \"pay more attention\" to samples from\n",
        "            an under-represented class.\n",
        "        sample_weight: Optional Numpy array of weights for\n",
        "            the training samples, used for weighting the loss function\n",
        "            (during training only). You can either pass a flat (1D)\n",
        "            Numpy array with the same length as the input samples\n",
        "            (1:1 mapping between weights and samples),\n",
        "            or in the case of temporal data,\n",
        "            you can pass a 2D array with shape\n",
        "            `(samples, sequence_length)`,\n",
        "            to apply a different weight to every timestep of every sample. This\n",
        "            argument is not supported when `x` is a dataset, generator, or\n",
        "           `keras.utils.Sequence` instance, instead provide the sample_weights\n",
        "            as the third element of `x`.\n",
        "        initial_epoch: Integer.\n",
        "            Epoch at which to start training\n",
        "            (useful for resuming a previous training run).\n",
        "        steps_per_epoch: Integer or `None`.\n",
        "            Total number of steps (batches of samples)\n",
        "            before declaring one epoch finished and starting the\n",
        "            next epoch. When training with input tensors such as\n",
        "            TensorFlow data tensors, the default `None` is equal to\n",
        "            the number of samples in your dataset divided by\n",
        "            the batch size, or 1 if that cannot be determined. If x is a\n",
        "            `tf.data` dataset, and 'steps_per_epoch'\n",
        "            is None, the epoch will run until the input dataset is exhausted.\n",
        "            When passing an infinitely repeating dataset, you must specify the\n",
        "            `steps_per_epoch` argument. This argument is not supported with\n",
        "            array inputs.\n",
        "        validation_steps: Only relevant if `validation_data` is provided and\n",
        "            is a `tf.data` dataset. Total number of steps (batches of\n",
        "            samples) to draw before stopping when performing validation\n",
        "            at the end of every epoch. If 'validation_steps' is None, validation\n",
        "            will run until the `validation_data` dataset is exhausted. In the\n",
        "            case of an infinitely repeated dataset, it will run into an\n",
        "            infinite loop. If 'validation_steps' is specified and only part of\n",
        "            the dataset will be consumed, the evaluation will start from the\n",
        "            beginning of the dataset at each epoch. This ensures that the same\n",
        "            validation samples are used every time.\n",
        "        validation_batch_size: Integer or `None`.\n",
        "            Number of samples per validation batch.\n",
        "            If unspecified, will default to `batch_size`.\n",
        "            Do not specify the `validation_batch_size` if your data is in the\n",
        "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
        "            (since they generate batches).\n",
        "        validation_freq: Only relevant if validation data is provided. Integer\n",
        "            or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
        "            If an integer, specifies how many training epochs to run before a\n",
        "            new validation run is performed, e.g. `validation_freq=2` runs\n",
        "            validation every 2 epochs. If a Container, specifies the epochs on\n",
        "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
        "            validation at the end of the 1st, 2nd, and 10th epochs.\n",
        "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
        "            input only. Maximum size for the generator queue.\n",
        "            If unspecified, `max_queue_size` will default to 10.\n",
        "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
        "            only. Maximum number of processes to spin up\n",
        "            when using process-based threading. If unspecified, `workers`\n",
        "            will default to 1. If 0, will execute the generator on the main\n",
        "            thread.\n",
        "        use_multiprocessing: Boolean. Used for generator or\n",
        "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
        "            threading. If unspecified, `use_multiprocessing` will default to\n",
        "            `False`. Note that because this implementation relies on\n",
        "            multiprocessing, you should not pass non-picklable arguments to\n",
        "            the generator as they can't be passed easily to children processes.\n",
        "\n",
        "    Unpacking behavior for iterator-like inputs:\n",
        "        A common pattern is to pass a tf.data.Dataset, generator, or\n",
        "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
        "      yield not only features (x) but optionally targets (y) and sample weights.\n",
        "      Keras requires that the output of such iterator-likes be unambiguous. The\n",
        "      iterator should return a tuple of length 1, 2, or 3, where the optional\n",
        "      second and third elements will be used for y and sample_weight\n",
        "      respectively. Any other type provided will be wrapped in a length one\n",
        "      tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
        "      should still adhere to the top-level tuple structure.\n",
        "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
        "      features, targets, and weights from the keys of a single dict.\n",
        "        A notable unsupported data type is the namedtuple. The reason is that\n",
        "      it behaves like both an ordered datatype (tuple) and a mapping\n",
        "      datatype (dict). So given a namedtuple of the form:\n",
        "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
        "      it is ambiguous whether to reverse the order of the elements when\n",
        "      interpreting the value. Even worse is a tuple of the form:\n",
        "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
        "      where it is unclear if the tuple was intended to be unpacked into x, y,\n",
        "      and sample_weight or passed through as a single element to `x`. As a\n",
        "      result the data processing code will simply raise a ValueError if it\n",
        "      encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
        "\n",
        "    Returns:\n",
        "        A `History` object. Its `History.history` attribute is\n",
        "        a record of training loss values and metrics values\n",
        "        at successive epochs, as well as validation loss values\n",
        "        and validation metrics values (if applicable).\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: 1. If the model was never compiled or,\n",
        "        2. If `model.fit` is  wrapped in `tf.function`.\n",
        "\n",
        "        ValueError: In case of mismatch between the provided input data\n",
        "            and what the model expects.\n",
        "    \"\"\"\n",
        "    _keras_api_gauge.get_cell('fit').set(True)\n",
        "    # Legacy graph support is contained in `training_v1.Model`.\n",
        "    version_utils.disallow_legacy_graph('Model', 'fit')\n",
        "    self._assert_compile_was_called()\n",
        "    self._check_call_args('fit')\n",
        "    _disallow_inside_tf_function('fit')\n",
        "\n",
        "    if validation_split:\n",
        "      # Create the validation data using the training data. Only supported for\n",
        "      # `Tensor` and `NumPy` input.\n",
        "      (x, y, sample_weight), validation_data = (\n",
        "          data_adapter.train_validation_split(\n",
        "              (x, y, sample_weight), validation_split=validation_split))\n",
        "\n",
        "    if validation_data:\n",
        "      val_x, val_y, val_sample_weight = (\n",
        "          data_adapter.unpack_x_y_sample_weight(validation_data))\n",
        "\n",
        "    with self.distribute_strategy.scope(), \\\n",
        "         training_utils.RespectCompiledTrainableState(self):\n",
        "      # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n",
        "      data_handler = data_adapter.DataHandler(\n",
        "          x=x,\n",
        "          y=y,\n",
        "          sample_weight=sample_weight,\n",
        "          batch_size=batch_size,\n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          initial_epoch=initial_epoch,\n",
        "          epochs=epochs,\n",
        "          shuffle=shuffle,\n",
        "          class_weight=class_weight,\n",
        "          max_queue_size=max_queue_size,\n",
        "          workers=workers,\n",
        "          use_multiprocessing=use_multiprocessing,\n",
        "          model=self,\n",
        "          steps_per_execution=self._steps_per_execution)\n",
        "\n",
        "      # Container that configures and calls `tf.keras.Callback`s.\n",
        "      if not isinstance(callbacks, callbacks_module.CallbackList):\n",
        "        callbacks = callbacks_module.CallbackList(\n",
        "            callbacks,\n",
        "            add_history=True,\n",
        "            add_progbar=verbose != 0,\n",
        "            model=self,\n",
        "            verbose=verbose,\n",
        "            epochs=epochs,\n",
        "            steps=data_handler.inferred_steps)\n",
        "\n",
        "      self.stop_training = False\n",
        "      train_function = self.make_train_function()\n",
        "      self._train_counter.assign(0)\n",
        "      callbacks.on_train_begin()\n",
        "      training_logs = None\n",
        "      # Handle fault-tolerance for multi-worker.\n",
        "      # TODO(omalleyt): Fix the ordering issues that mean this has to\n",
        "      # happen after `callbacks.on_train_begin`.\n",
        "      data_handler._initial_epoch = (  # pylint: disable=protected-access\n",
        "          self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n",
        "      for epoch, iterator in data_handler.enumerate_epochs():\n",
        "        self.reset_metrics()\n",
        "        callbacks.on_epoch_begin(epoch)\n",
        "        with data_handler.catch_stop_iteration():\n",
        "          for step in data_handler.steps():\n",
        "            with trace.Trace(\n",
        "                'TraceContext',\n",
        "                graph_type='train',\n",
        "                epoch_num=epoch,\n",
        "                step_num=step,\n",
        "                batch_size=batch_size):\n",
        "              callbacks.on_train_batch_begin(step)\n",
        "              tmp_logs = train_function(iterator)\n",
        "              if data_handler.should_sync:\n",
        "                context.async_wait()\n",
        "              logs = tmp_logs  # No error, now safe to assign to logs.\n",
        "              end_step = step + data_handler.step_increment\n",
        "              callbacks.on_train_batch_end(end_step, logs)\n",
        "        epoch_logs = copy.copy(logs)\n",
        "\n",
        "        # Run validation.\n",
        "        if validation_data and self._should_eval(epoch, validation_freq):\n",
        "          # Create data_handler for evaluation and cache it.\n",
        "          if getattr(self, '_eval_data_handler', None) is None:\n",
        "            self._eval_data_handler = data_adapter.DataHandler(\n",
        "                x=val_x,\n",
        "                y=val_y,\n",
        "                sample_weight=val_sample_weight,\n",
        "                batch_size=validation_batch_size or batch_size,\n",
        "                steps_per_epoch=validation_steps,\n",
        "                initial_epoch=0,\n",
        "                epochs=1,\n",
        "                max_queue_size=max_queue_size,\n",
        "                workers=workers,\n",
        "                use_multiprocessing=use_multiprocessing,\n",
        "                model=self,\n",
        "                steps_per_execution=self._steps_per_execution)\n",
        "          val_logs = self.evaluate(\n",
        "              x=val_x,\n",
        "              y=val_y,\n",
        "              sample_weight=val_sample_weight,\n",
        "              batch_size=validation_batch_size or batch_size,\n",
        "              steps=validation_steps,\n",
        "              callbacks=callbacks,\n",
        "              max_queue_size=max_queue_size,\n",
        "              workers=workers,\n",
        "              use_multiprocessing=use_multiprocessing,\n",
        "              return_dict=True)\n",
        "          val_logs = {'val_' + name: val for name, val in val_logs.items()}\n",
        "          epoch_logs.update(val_logs)\n",
        "\n",
        "        callbacks.on_epoch_end(epoch, epoch_logs)\n",
        "        training_logs = epoch_logs\n",
        "        if self.stop_training:\n",
        "          break\n",
        "\n",
        "      # If eval data_hanlder exists, delete it after all epochs are done.\n",
        "      if getattr(self, '_eval_data_handler', None) is not None:\n",
        "        del self._eval_data_handler\n",
        "      callbacks.on_train_end(logs=training_logs)\n",
        "      return self.history\n",
        "File:      /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\n",
        "Type:      method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izi_7GySBipH"
      },
      "source": [
        "??model.fit for # help  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPY6o_zSGP9N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}